{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b781797a",
   "metadata": {},
   "source": [
    "# [Harnessing Power at the Edge: An Introduction to Local Large Language Models](https://pyimagesearch.com/2024/05/13/harnessing-power-at-the-edge-an-introduction-to-local-large-language-models/)\n",
    "\n",
    "## About LLMs\n",
    "\n",
    "- Large Language Models (LLMs): AI systems trained on extensive text datasets.\n",
    "  - Transformers architecture (self-attention mechanisms; **context**)\n",
    "\n",
    "- [Attention Is All You Need (Google)](https://arxiv.org/abs/1706.03762) - Introduced understanding context, semantics and Transformers\n",
    "- GPT-1 was notable for its decoder-only architecture and its pioneering approach to generative pre-training\n",
    "\n",
    "## Training LLMs\n",
    "\n",
    "Essential steps with LLMs:\n",
    "- Pre-training: training over a large corpus, the model understands patterns and acquires general knowledge\n",
    "- Fine-tuning: re-train over a small corpus and update weights (frozen essential layers) for specific task  \n",
    "\n",
    "## Applications\n",
    "\n",
    "Content generation, customar service, programming and code generation, translation and localization, education and tutoring, and more.\n",
    "\n",
    "## Ethical Considerations\n",
    "\n",
    "Problems: data privacy (specially considering cloud-based services), model bias, and the generation of misleading information.\n",
    "\n",
    "## Local LLMs\n",
    "\n",
    "Cloud-based services for LLMs -> LLMs on local infrastructures\n",
    "\n",
    "This movement is driven by some factors, like the need for lower latency, and the desire for greater control over the models.\n",
    "\n",
    "## Advantages of Local LLMs\n",
    "\n",
    "Data privacy and security, low latency, cost, always-on availability.\n",
    "\n",
    "## Considerations\n",
    "\n",
    "- Significant hardware investments are necessary (computation-heavy workload associated)\n",
    "- Necessity of manage and update the models, handle data security, and ensure the infrastructure’s integrity.\n",
    "- Scaling local hardware can be challenging and expensive compared to scalable cloud solutions.\n",
    "- There are numerous frameworks that facilitate the access and use of this models\n",
    "\n",
    "Organizations can choose the best strategy for their needs (using cloud-based services or locally).\n",
    "\n",
    "## Common Model Formats for LLMs\n",
    "\n",
    "### PyTorch Models\n",
    "\n",
    "- `.pt`, `.pth`\n",
    "- `fp16`, `fp32` denotes the precision of the model's floating-point computations (there are a trade-off between model's memory and computational speed)\n",
    "\n",
    "### SafeTensors\n",
    "\n",
    "- Created for bolster the security and integrity of model data\n",
    "- SafeTensors has demonstrated faster performance than PyTorch on both CPU and GPU environments\n",
    "\n",
    "### GGML and GGUF\n",
    "\n",
    "- GGML (GPT-Generated Model Language) enables LLMs to run efficiently on consumer-grade CPUs -> quantization techniques\n",
    "- GGUF (GPT-Generated Unified Format) was designed to be more flexible and robust, supporting a broader array of models\n",
    "\n",
    "## Post-training Quantization\n",
    "\n",
    "### GPTQ (Generalized Post-Training Quantization)\n",
    "\n",
    "- Compress all model weights to 4-bit quantization; during inference, dynamically dequantizes weights to float16\n",
    "- There are two directions in reducing computational demands of GPTs:\n",
    "  - develop smaller models\n",
    "  - reduce size of existing models\n",
    "\n",
    "- AutoGPTQ Library\n",
    "\n",
    "### AWQ (Activation-aware Weight Quantization)\n",
    "\n",
    "- This method dynamically adjusts the quantization levels of weights to balance performance with model accuracy\n",
    "- Distinguishes itself from other methods by giving importance to individual weights within a model (premise: not all weights contribute equally to a model’s performance)\n",
    "\n",
    "- AutoAWQ\n",
    "\n",
    "## Frameworks for Local LLMs\n",
    "\n",
    "- Ollama\n",
    "- LM Studio\n",
    "- NVIDIA ChatRTX"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
